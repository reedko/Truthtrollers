// backend/src/core/scrapeReference.js
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Process an AI reference using pre-fetched text
// (avoiding double-fetch from evidence engine)
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import logger from "../utils/logger.js";
import * as cheerio from "cheerio";
import { extractAuthors } from "../utils/extractAuthors.js";
import { extractPublisher } from "../utils/extractPublisher.js";
import { getMainHeadline } from "../utils/getMainHeadline.js";
import { createContentInternal } from "../storage/createContentInternal.js";
import { getBestImage } from "../utils/getBestImage.js";

/**
 * scrapeReference(query, { url, raw_text, raw_html, title, authors, taskContentId })
 *
 * Processes a reference using pre-fetched text/HTML:
 * - Parse HTML with cheerio (or use raw_text for PDFs)
 * - Extract metadata (authors, publisher, title)
 * - Create reference content row
 * - Returns: { referenceContentId, url, title, text, authors, publisher }
 *
 * NOTE: Claims extraction happens separately via processReferenceClaims
 */
export async function scrapeReference(query, { url, raw_text, raw_html, title, authors: providedAuthors, taskContentId }) {
  try {
    logger.log(`ğŸŸ¦ [scrapeReference] Processing reference: ${url}`);

    // Accept either raw_html or raw_text (raw_html takes precedence)
    const htmlContent = raw_html || raw_text;

    if (!htmlContent || htmlContent.length < 100) {
      logger.warn(`âš ï¸ [scrapeReference] Insufficient HTML/text for ${url}`);
      return null;
    }

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 1. PARSE HTML (skip parsing if we only have plain text from PDF)
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    const $ = raw_html ? cheerio.load(htmlContent) : null;

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 2. EXTRACT METADATA: title, authors, publisher
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    // Use provided title (from PDF metadata) or extract from HTML/text
    let finalTitle = title || ($? await getMainHeadline($) : null) || "AI Reference";

    // If no title and we have raw_text (PDF), extract from first line
    if ((!finalTitle || finalTitle === "AI Reference" || finalTitle.length < 3) && raw_text && !raw_html) {
      const lines = raw_text.split('\n').map(l => l.trim()).filter(Boolean);
      // Use first substantial line as title (skip very short lines)
      for (const line of lines) {
        if (line.length > 10 && line.length < 200) {
          finalTitle = `[PDF] ${line}`;
          break;
        }
      }
    }

    if (!finalTitle || finalTitle.length < 3) {
      finalTitle = ($? await getMainHeadline($) : null) || "AI Reference";
    }

    // Use provided authors (from PDF metadata) or extract from HTML
    const authors = providedAuthors || ($ ? await extractAuthors($) : []);
    const publisher = $ ? await extractPublisher($) : null;

    // Extract thumbnail
    let thumbnail = "";
    const isPdf = !raw_html && raw_text; // PDF if we have text but no HTML

    if (isPdf) {
      // TODO: For PDFs scraped from extension, we need to pass the blob to thumbnail generator
      // For now, skip thumbnail generation (would need to refetch PDF which might be blocked)
      logger.log(`ğŸ–¼ï¸  [scrapeReference] Skipping PDF thumbnail (would require re-fetching blocked PDF)`);
    } else if ($) {
      // For HTML pages, extract image from page
      thumbnail = getBestImage($, url) || "";
      if (thumbnail) {
        logger.log(`ğŸ–¼ï¸  [scrapeReference] Extracted thumbnail: ${thumbnail.slice(0, 80)}...`);
      }
    }

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 3. EXTRACT CLEAN TEXT
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    let text;
    if (raw_html) {
      // Extract clean text from HTML
      $("script, style, link").remove();
      let cleanText = $.text().trim();
      if (cleanText.length > 60000) {
        cleanText = cleanText.slice(0, 60000);
      }
      text = cleanText;
    } else {
      // Use raw_text directly (for PDFs)
      text = raw_text.slice(0, 60000);
    }

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 4. CREATE REFERENCE CONTENT ROW
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    const referenceContentId = await createContentInternal(query, {
      content_name: finalTitle,
      url,
      media_source: publisher?.name || "Unknown",
      topic: "AI Evidence", // References inherit task topic in UI
      subtopics: [],
      content_type: "reference",
      taskContentId, // Link to parent task
      thumbnail,
      details: text.slice(0, 500), // Preview
    });

    logger.log(
      `âœ… [scrapeReference] Created reference content_id=${referenceContentId} for ${url}`
    );

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 5. RETURN STRUCTURED OUTPUT
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    return {
      referenceContentId,
      url,
      title: finalTitle,
      text,
      authors,
      publisher,
    };
  } catch (err) {
    logger.error("âŒ [scrapeReference] Fatal error on:", url, err);
    return null;
  }
}
