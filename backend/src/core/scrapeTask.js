// backend/src/core/scrapeTask.js
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// STEP 1 of the TruthTrollers pipeline:
// Fetch readable text, metadata, DOM refs, inline refs
// Create/Persist the TASK content row (content_type = 'task')
// NO CLAIM EXTRACTION here
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import logger from "../utils/logger.js";
import { fetchExternalPageContent } from "../utils/fetchExternalPageContent.js";
import { fetchPageContent } from "../utils/fetchPageContent.js";
import { extractAuthors } from "../utils/extractAuthors.js";
import { extractPublisher } from "../utils/extractPublisher.js";
import { extractReferences } from "../utils/extractReferences.js";
import { extractInlineRefs } from "../utils/extractInlineRefs.js";
import { extractTestimonialsFromHtml } from "../utils/extractTestimonials.js";
import { extractTranscript } from "./youtubeTranscript.js";
import { getMainHeadline } from "../utils/getMainHeadline.js";
import { getBestImage } from "../utils/getBestImage.js";
import { persistTaskContent } from "../storage/persistContentAndEvidence.js";
import * as cheerio from "cheerio";

/**
 * scrapeTask(query, url, raw_html?, mediaSource?, providedAuthors?)
 *  â€¢ Fetch HTML, PDF, or YouTube transcript (OR use provided raw_html)
 *  â€¢ Extract: text, title, authors, publisher, thumbnail
 *  â€¢ Extract DOM references
 *  â€¢ Extract inline references from text
 *  â€¢ Persist the task content row in DB
 *  â€¢ Returns: { taskContentId, text, metadata, domRefs, inlineRefs }
 */
export async function scrapeTask(query, url, raw_html = null, mediaSource = null, providedAuthors = null) {
  try {
    logger.log(`ðŸŸ¦ [scrapeTask] Starting scrape for: ${url}`);
    if (mediaSource) {
      logger.log(`ðŸ“Œ [scrapeTask] Media source hint: ${mediaSource}`);
    }
    if (providedAuthors && providedAuthors.length > 0) {
      logger.log(`ðŸ‘¤ [scrapeTask] Using ${providedAuthors.length} provided author(s):`, providedAuthors);
    }

    let $ = null;
    let text = "";
    let rawHtml = "";
    let title = "";
    let authors = providedAuthors || []; // Use provided authors if available
    let publisher = mediaSource; // Use hint if provided, will be overridden if extracted
    let thumbnail = "";
    let domRefs = [];
    let inlineRefs = [];
    let isPdf = /\.pdf($|\?)/i.test(url);

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 1. FETCH CONTENT (or use provided HTML)
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    // Use provided HTML if available (from extension's current page DOM)
    if (raw_html) {
      logger.log(`âœ… [scrapeTask] Using provided HTML (${raw_html.length} chars, no fetch!)`);
      $ = cheerio.load(raw_html);
      rawHtml = raw_html;
    }
    // Otherwise fetch from URL
    else if (!isPdf) {
      try {
        $ = await fetchPageContent(url);
        rawHtml = $.html();
      } catch (err) {
        logger.warn("âš ï¸ fetchPageContent failed, trying external:", err);
      }
    }

    if (!$ && !raw_html) {
      const ext = await fetchExternalPageContent(url);
      if (!ext || !ext.$) {
        logger.warn("âš ï¸ No usable content. aborting:", url);
        return null;
      }
      $ = ext.$;
      rawHtml = $.html();

      // PDF metadata
      if (ext.pdfMeta) {
        if (ext.pdfMeta.title) title = ext.pdfMeta.title;
        if (ext.pdfMeta.thumbnailUrl) thumbnail = ext.pdfMeta.thumbnailUrl;

        if (ext.pdfMeta.authors?.length) {
          authors = ext.pdfMeta.authors.map((a) => ({
            name: a,
            description: null,
            image: null,
          }));
        }
      }
    }

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 2. EXTRACT READABLE TEXT
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    // YouTube transcripts
    const isYouTube = url.includes("youtube.com") || url.includes("youtu.be");
    if (isYouTube) {
      const transcript = await extractTranscript(url);
      if (transcript) text = transcript;
    }

    // fallback: get readable text from HTML
    if (!text) {
      const cleanHtml = cleanForReadability(rawHtml);
      const $clean = cheerio.load(cleanHtml);

      let extracted = $clean.text().trim();
      if (extracted.length > 60000) extracted = extracted.slice(0, 60000);

      text = extracted;
    }

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 3. EXTRACT METADATA: title, authors, publisher
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    if (!title || title.length < 3) {
      title = (await getMainHeadline($)) || "Untitled Article";
    }

    // Only extract authors from HTML if not already provided
    if (!providedAuthors || providedAuthors.length === 0) {
      const htmlAuthors = await extractAuthors($);
      authors = mergeAuthors(authors, htmlAuthors);
    } else {
      logger.log(`âœ… [scrapeTask] Skipping HTML author extraction (using provided authors)`);
    }

    // Only extract publisher if not already provided via mediaSource
    if (!publisher) {
      publisher = await extractPublisher($);
    }

    // Extract thumbnail if not already set (from PDF)
    if (!thumbnail) {
      thumbnail = getBestImage($, url) || "";
      if (thumbnail) {
        logger.log(`ðŸ–¼ï¸  [scrapeTask] Extracted thumbnail: ${thumbnail.slice(0, 80)}...`);
      }
    }

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 4. REFERENCES
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    domRefs = await extractReferences($);
    inlineRefs = extractInlineRefs(text);

    // dedupe inline/DOM duplicates
    const seen = new Set(domRefs.map((r) => r.url));
    inlineRefs = inlineRefs.filter((r) => !seen.has(r.url));

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 5. PERSIST TASK CONTENT ROW (NO CLAIMS YET)
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    const taskContentId = await persistTaskContent(query, {
      url,
      title,
      rawText: text,
      publisher: publisher?.name || null,
      authors, // persisted in child method
      thumbnail,
    });

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // 6. RETURN STRUCTURED TASK SCRAPE OUTPUT
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    return {
      taskContentId,
      url,
      title,
      text,
      authors,
      publisher,
      thumbnail,
      domRefs,
      inlineRefs,
      rawHtml,
    };
  } catch (err) {
    logger.error("âŒ [scrapeTask] Fatal error on:", url, err);
    return null;
  }
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// UTILITIES
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function cleanForReadability(html) {
  if (!html) return "";
  const $ = cheerio.load(html);
  $("script, style, link").remove();
  return $.html();
}

function mergeAuthors(a1, a2) {
  const out = [];
  const seen = new Set();

  [...a1, ...a2].forEach((a) => {
    const name = a?.name?.trim();
    if (name && !seen.has(name)) {
      out.push(a);
      seen.add(name);
    }
  });

  return out;
}
